---
format: 
  revealjs:
    embed-resources: true # note, chackboard option, even commented, seemed to be an issue with embed-resources..., see https://claudiu.psychlab.eu/post/revealjs-presentations-with-quarto-a-template/
    transition: slide 
    slide-number: c
    progress: true
    theme: [default] # see comment in the custom.scss file
    #css: fonts.css # needed to import the Right Grotesk Compact font
    #theme: default
    margin: 0.1
    width: 1200
    output-file: coherens_workshop.html
    smaller: true
editor: source 
  #render-on-save: true
from: markdown+emoji
execute: 
  cache: true # need to put it to false when there is an update of data WITHOUT a change of code
---

# COHERENS workshop #2{background-color="#cc785cff"}

Florian Ricour (ECOMOD) <br>
Ludovic Lepers (MFC)

# Part I {background-color="#cc785cff"}

::: {style="position:absolute;top:70%;left:0%;font-size:40px;"}
JupyterHub on ECMWF
::: 

## A clear procedure to get there

- Need an ECMWF account
- Access to local terminal
  1. `tsh login --proxy=jump.ecmwf.int`
  2. `ssh -X hpc-login`
- [Tutorial](https://naturalsciences.sharepoint.com/sites/ECOMOD/Shared%20Documents/Forms/AllItems.aspx?id=%2Fsites%2FECOMOD%2FShared%20Documents%2FECMWF%2Fjupyterlab%5Fecmwf%2Epdf&parent=%2Fsites%2FECOMOD%2FShared%20Documents%2FECMWF) to connect to JupyterHub
- Multiple server [options](https://confluence.ecmwf.int/display/UDOC/Configuring+your+Jupyter+session+on+HPCF+or+ECS) (Profile, CPU number, session duration, ...)

## Let's try [JupyterHub](https://jupyter.org/hub)!

- Demo on [ECMWF JupyterHub](https://jupyterhub.ecmwf.int)
  - Notebook
  - General structure

- HPC [Filesystems](https://confluence.ecmwf.int/display/UDOC/HPC2020%3A+Filesystems)

```{r}
#| label: file-system-table
#| echo: false
#| message: false
#| warning: false
file_system <- c('HOME', 'PERM', 'HPCPERM', 'SCRATCH', 'TMPDIR')
features <- c('Backed up', 'No back up', 'No back up', 'No back up', 'Deleted at the end of session/job')
quota <- c('10 GB', '500 GB', '100 GB*/1 TB', '50 TB*/2 TB', '3 GB by default')

library(tibble)
library(gt)

tb <- tibble(file_system, features, quota)
# Create and format the table
gt_table <- tb %>%
  gt() %>%
  cols_label(
    file_system = "File System",
    features = "Features",
    quota = "Quota"
  ) %>%
  tab_style(
    style = cell_fill(color = "#f0f0f0"),
    locations = cells_body(rows = seq(1, nrow(tb), 2))
  ) %>%
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels()
  ) %>%
  tab_style(
    style = cell_text(color = "navy"),
    locations = cells_body(columns = "file_system")
  ) %>%
  tab_options(
    column_labels.background.color = "#e0e0e0",
    table.border.top.style = "hidden",
    table.border.bottom.style = "hidden"
  ) |>
  tab_header(
    title = "File System Features"
  ) |>
  tab_source_note(
    source_note = "* for users without HPC access such as ECS"
  ) |>
  opt_css(
  css = "
    .gt_table {
      margin-left: 45px !important;
      margin-right: auto !important;
    }
  "
)

# Display the table
gt_table
```

## Conclusion on Part I

::: {.incremental}

- GUI is more user-friendly than a cold heartless terminal
- Take advantage of the HPC filesystem at your disposal
- Recent service (April 2024), you will look so damn cool !
- Session up to 7 days, just go the URL and you're back online
- Use it for other tasks than model simulations
- Save time from heavy downloads/uploads from/to Sharepoint (Erk)
- Preserve your laptop and run big python/R codes on ECMWF
- You want to go home but need to wait for a script to finish ? $\rightarrow$ ECMWF
- I like it and I hope you'll use it !
:::


# Short break {background-color="#cc785cff"}

# Part II {background-color="#cc785cff"}

::: {style="position:absolute;top:70%;left:0%;font-size:40px;"}
Artificial Intelligence, how smart is it?
:::

## Who hasn't used ChatGPT here?

::: {.fragment .fade-in-then-out}
<!--![](https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExMjVwdHdtemhlMmdqOGJtY3oyemQ3cXMwMjJ1OG9hNmg4MWxvdHY0byZlcD12MV9naWZzX3NlYXJjaCZjdD1n/EouEzI5bBR8uk/giphy.gif)-->
![](https://media.giphy.com/media/j4lJOuwvAzyRcnWrFi/giphy.gif?cid=790b76114ggiswqfx07dbvx906s4qfy2h30e3j96dvw3qeul&ep=v1_gifs_search&rid=giphy.gif&ct=g)
:::

::: {.fragment .fade-in}
::: {style="position:absolute;top:20%;left:0%;font-size:30px;"}
* chatGPT (you know it)
* Claude.ai (20$/month well spent)
* GitHub Copilot (code completion)
* Perplexity (search engine)
* NotebookLM (nice podcasts)
* DeepL (translation)
* Many more
:::
:::

::: {.fragment .fade-in}
::: {style="position:absolute;top:40%;left:50%;"}
<span style="background-color:#cc785cff; color:white; padding:10px 20px; border-radius:15px; display:inline-block; font-size:40px;">Artificial intelligence</span>
:::
:::

## AI is a vague terminology

[![](images/what_is_ai.png)](https://cgarbin.github.io/deep-learning-for-image-processing-overview/){fig-align="left"}

::: {.fragment .fade-in}
::: {style="position:absolute;top:30%;left:55%;font-size:40px;"}
![](images/ann.jpg)
<strong>Artificial Neural Network (ANN)</strong><br>
:::
:::

## A simple ANN - The Perceptron

Based on an artificial neuron called **threshold logic unit** (TLU)

::: {style="position:absolute;top:20%;left:-10%;font-size:40px;"}
![](images/tlu.jpg){width=800}
:::


::: {style="position:absolute;top:40%;left:60%;font-size:30px;"}
$$
\text{heaviside}(z) = 
\begin{cases}
0 & \text{if } z < 0 \\
1 & \text{if } z \geq 0
\end{cases}
$$
:::

## Perceptron composed of one or more TLUs

Every TLU connected to every input = **fully connected layer** or **dense layer**

::: {style="position:absolute;top:20%;left:0%;font-size:40px;"}
![](images/more_tlus.jpg){width=600}
:::

::: {style="position:absolute;top:25%;left:50%;font-size:30px;"}
* $h_{\mathbf{W},\mathbf{b}}(\mathbf{X}) = \phi(\mathbf{X}\mathbf{W} + \mathbf{b})$
* $\mathbf{b} = \text{bias vector}, \text{one value per neuron}$
* $\phi = \text{activation function}$
:::

::: notes
Purpose of bias neurons:

The main purpose of a bias neuron is to shift the activation function left or right, which can be crucial for successful learning. Here's why this is important:
* Flexibility: Bias allows the model to fit the data better by shifting the activation function. Without bias, the function would always pass through the origin (0,0).
* Learning power: Bias increases the capacity of the network to learn complex patterns by providing an additional parameter to adjust.
* Default activation: Bias allows a neuron to have a default "firing" value even when all inputs are zero.
:::

::: {style="position:absolute;top:45%;left:50%;font-size:30px;"}
![](images/activation.jpg){width=1200}
:::

## Multilayer Perceptron - XOR example

::: {style="position:absolute;top:15%;left:0%;font-size:40px;"}
![](images/multilayer_perceptron.jpg){width=600}
:::

::: {style="position:absolute;top:15%;left:55%;font-size:40px;"}
![](images/xor.jpg){width=600}
:::

::: {style="position:absolute;top:80%;left:55%;font-size:20px;"}
ANN with deep stack of hidden layers = **deep neural** network
:::

## Tweaking parameters to minimize the cost

Famous optimization algorithm - gradient descent (iterative process)

::: {.fragment .fade-in-then-out}
::: {style="position:absolute;top:20%;left:0%;font-size:40px;"}
![](images/cost_function.jpg){width=600}
:::

::: {style="position:absolute;top:25%;left:50%;font-size:30px;"}
* $\boldsymbol{\theta} = \text{parameter vector}$
* $\eta = \text{learning step} = \text{learning rate}$
* $\boldsymbol{\theta}^{(\text{next step})} = \boldsymbol{\theta} - \eta \nabla_{\boldsymbol{\theta}} \text{Cost}(\boldsymbol{\theta})$
* $\text{If Cost}(\boldsymbol{\theta}) = \text{RMSE}(\boldsymbol{\theta})$ 
  * $\text{min Cost}(\boldsymbol{\theta}) = min \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2}$
:::
:::

::: {.fragment .fade-in}
::: {style="position:absolute;top:20%;left:0%;font-size:40px;"}
![](images/early_stop.jpg){width=600}
:::

::: {style="position:absolute;top:25%;left:50%;font-size:30px;"}
* $\boldsymbol{\theta} = \text{parameter vector}$
* $\eta = \text{learning step} = \text{learning rate}$
* $\boldsymbol{\theta}^{(\text{next step})} = \boldsymbol{\theta} - \eta \nabla_{\boldsymbol{\theta}} \text{Cost}(\boldsymbol{\theta})$
* $\text{If Cost}(\boldsymbol{\theta}) = \text{RMSE}(\boldsymbol{\theta})$ 
  * $\text{min Cost}(\boldsymbol{\theta}) = min \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2}$
:::
:::

## Have you ever seen an ANN in action?

[TensorFlow Playground](https://playground.tensorflow.org/){preview-link="true"}

## Now that we know you've all used chatGPT

::: {style="position:absolute;top:20%;left:0%;font-size:30px;"}
* chatGPT (you know it)
* Claude.ai (20$/month well spent)
* GitHub Copilot (code completion)
* Perplexity (search engine)
* NotebookLM (nice podcasts)
* DeepL (translation)
* Many more
:::

::: {.fragment .fade-in}
::: {style="position:absolute;top:40%;left:50%;"}
<span style="background-color:#cc785cff; color:white; padding:10px 20px; border-radius:15px; display:inline-block; font-size:40px;">Large Language Models (LLMs)</span>
:::
:::


# Part III {background-color="#cc785cff"}

::: {style="position:absolute;top:70%;left:0%;font-size:40px;"}
LLM, What's all the fuss about?
::: 

<!-- Ludo's part -->
 
## Large Langage Model (LLM)s ?
::: {style="position:absolute;top:20%;left:0%;font-size:30px;"}
  - Specialized in langage processing &rarr; Working with text written by human
  - Large Langage Model &rarr; Huge model with **billions** of parameters (GPT3 had 175 billion parameters)
  - The big ones are proprietary &rarr; we do not know the detail (some are "open-weigth, not the same as "open-source")
:::


## How do they works ? Transforming a text in something a computer can understand
 
- Tokenization : Splitting the text into "tokens". 
- Example : <https://tiktokenizer.vercel.app/?model=text-davinci-003>

. . .

::: {.incremental}
- Vectorization: Each token receive a vector (GPT3 => 12 288 dimension)
- This vector has a sementic meaning (Famous example : `King - Man + Women = Queen`)
- These are trained too 
:::

## How do they works ? Transfomer principle

- GPT => Generative pretrained transfomers
- Neural network with many layer. 
- The model as as input a series of token and as output a token called "context window"
- All the token pass trought all of them and are changed based on the others
- "A black dog" &rarr; dog vector will be modified to account for the fact that the dog is black (simplification) 
- At the end the model produce a vector transformed in a probability distribution. A token is chosen from there.
- When the previous token is added, the then new "context window" can be used a new input => text can be generated "forever"
- This is how the model is trained => try to predict the next token, then backpropagation => TRAINED TO DO THE MOST PROBABLE, NOT THE MOST CORRECT
- Kinds of "knowledge" => the one inside of the weigth vs the one in the context window(!limited in size)
EXAMPLE INSIDE AND OUTSIDE KNOWLEDGE
- Usually the model is better at retrieving information at the end and the start of the context window
- LLM are then usable for many tasks (code, chatbot)

## Example of a chatbot

- "Fine tuning" the model => Reinforcement learning human feedback (RLHF)
- Human looks at many answer and says which one are good and which ones are bad
- This prevent the model to says "dangerous" stuff, but reduce its capacities a bit => it has also been showed to privilegie human manipulation about exactitude
MODELS ARE TRAINED TO GIVE NICE LOOKING ANSWER TO HUMAN QUESTIONS, AND THEY HAVE BECCOME GOOD AT IT...
- Then the model is told to "simulate" a conversion between a virtual assisstant and a human. There is some instruction (invisible) at the start of the conversation
- It was/is possible sometime to jailbrake the instruction (example with torrent website)


# Part IV {background-color="#cc785cff"}

::: {style="position:absolute;top:70%;left:0%;font-size:40px;"}
Examples and best practices
::: 

## The less the model has to guess the better

[Non exhaustive list](https://platform.openai.com/docs/guides/prompt-engineering/six-strategies-for-getting-better-results){preview-link="false"}

::: {.incremental}
* Write clear instructions
* Provide reference text and/or code 
* Split complex tasks into simpler subtasks
* Give the model time to "think" by asking for a "chain of thought"
* Tell the model what to do, rather than what not to do
* ...
:::

## Some other example

::: {.incremental}
* Bing chat can go on the internet
* 
:::

## Generating image from text (or image)

- cf bing chat, Dall-e, midjourney
- openweigth models exist: flux, stable diffusion... ! needs a decent GPU to have image generates in less than 10 minutes
- Videos models are quickly improving

## Generating music from text

- Suno, Udio...
- cf COHERENS rap


## It feels like coding with 4 hands



## Shiny app built from scratch 

Amazingly fast.

![](gif/shiny.gif)

# Time to eat ! {background-color="#cc785cff"}