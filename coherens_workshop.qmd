---
format: 
  revealjs:
    embed-resources: true # note, chackboard option, even commented, seemed to be an issue with embed-resources..., see https://claudiu.psychlab.eu/post/revealjs-presentations-with-quarto-a-template/
    transition: slide 
    slide-number: c
    progress: true
    theme: [default] # see comment in the custom.scss file
    #css: fonts.css # needed to import the Right Grotesk Compact font
    #theme: default
    margin: 0.1
    width: 1200
    output-file: coherens_workshop.html
    smaller: true
editor: source 
  #render-on-save: true
from: markdown+emoji
execute: 
  cache: true # need to put it to false when there is an update of data WITHOUT a change of code
---

# COHERENS workshop #2 {background-color="#335c67"}

Florian Ricour (ECOMOD) <br>
Ludovic Lepers (MFC)

::: footer
"Information is current as of November 24, 2024, and subject to change."
:::

# Part I {background-color="#335c67"}

::: {style="position:absolute;top:70%;left:0%;font-size:40px;"}
JupyterHub on ECMWF
::: 

## A clear procedure to get there

- Need an ECMWF account
- Access to local terminal
  1. `tsh login --proxy=jump.ecmwf.int`
  2. `ssh -X hpc-login`
- [Tutorial](https://naturalsciences.sharepoint.com/sites/ECOMOD/Shared%20Documents/Forms/AllItems.aspx?id=%2Fsites%2FECOMOD%2FShared%20Documents%2FECMWF%2Fjupyterlab%5Fecmwf%2Epdf&parent=%2Fsites%2FECOMOD%2FShared%20Documents%2FECMWF) to connect to JupyterHub
- Multiple server [options](https://confluence.ecmwf.int/display/UDOC/Configuring+your+Jupyter+session+on+HPCF+or+ECS) (Profile, CPU number, session duration, ...)

## Let's try [JupyterHub](https://jupyter.org/hub)!

- Demo on [ECMWF JupyterHub](https://jupyterhub.ecmwf.int)
  - Notebook
  - General structure

- HPC [Filesystems](https://confluence.ecmwf.int/display/UDOC/HPC2020%3A+Filesystems)

```{r}
#| label: file-system-table
#| echo: false
#| message: false
#| warning: false
file_system <- c('HOME', 'PERM', 'HPCPERM', 'SCRATCH', 'TMPDIR')
features <- c('Backed up', 'No back up', 'No back up', 'No back up', 'Deleted at the end of session/job')
quota <- c('10 GB', '500 GB', '100 GB*/1 TB', '50 TB*/2 TB', '3 GB by default')

library(tibble)
library(gt)

tb <- tibble(file_system, features, quota)
# Create and format the table
gt_table <- tb %>%
  gt() %>%
  cols_label(
    file_system = "File System",
    features = "Features",
    quota = "Quota"
  ) %>%
  tab_style(
    style = cell_fill(color = "#f0f0f0"),
    locations = cells_body(rows = seq(1, nrow(tb), 2))
  ) %>%
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels()
  ) %>%
  tab_style(
    style = cell_text(color = "navy"),
    locations = cells_body(columns = "file_system")
  ) %>%
  tab_options(
    column_labels.background.color = "#e0e0e0",
    table.border.top.style = "hidden",
    table.border.bottom.style = "hidden"
  ) |>
  tab_header(
    title = "File System Features"
  ) |>
  tab_source_note(
    source_note = "* for users without HPC access such as ECS"
  ) |>
  opt_css(
  css = "
    .gt_table {
      margin-left: 45px !important;
      margin-right: auto !important;
    }
  "
)

# Display the table
gt_table
```

## Conclusion on Part I

::: {.incremental}

- GUI is more user-friendly than a cold heartless terminal
- Take advantage of the HPC filesystem at your disposal
- Recent service (April 2024), you will look so damn cool !
- Session up to 7 days, just go the URL and you're back online
- Use it for other tasks than model simulations
- Save time from heavy downloads/uploads from/to Sharepoint (Erk)
- Preserve your laptop and run big python/R codes on ECMWF
- You want to go home but need to wait for a script to finish ? $\rightarrow$ ECMWF
- I like it and I hope you'll use it !
:::


# Short break {background-color="#335c67"}

# Part II {background-color="#335c67"}

::: {style="position:absolute;top:70%;left:0%;font-size:40px;"}
Artificial Intelligence, how smart is it?
:::

## Who hasn't used ChatGPT here?

::: {.fragment .fade-in-then-out}
<!--![](https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExMjVwdHdtemhlMmdqOGJtY3oyemQ3cXMwMjJ1OG9hNmg4MWxvdHY0byZlcD12MV9naWZzX3NlYXJjaCZjdD1n/EouEzI5bBR8uk/giphy.gif)-->
![](https://media.giphy.com/media/j4lJOuwvAzyRcnWrFi/giphy.gif?cid=790b76114ggiswqfx07dbvx906s4qfy2h30e3j96dvw3qeul&ep=v1_gifs_search&rid=giphy.gif&ct=g)
:::

::: {.fragment .fade-in}
::: {style="position:absolute;top:20%;left:0%;font-size:30px;"}
* chatGPT (you know it)
* Claude.ai (20$/month well spent)
* GitHub Copilot (code completion)
* Perplexity (search engine)
* NotebookLM (nice podcasts)
* DeepL (translation)
* Many more
:::
:::

::: {.fragment .fade-in}
::: {style="position:absolute;top:40%;left:50%;"}
<span style="background-color:#335c67; color:white; padding:10px 20px; border-radius:15px; display:inline-block; font-size:40px;">Artificial intelligence</span>
:::
:::

## AI is a vague terminology

::: {style="position:absolute;top:10%;left:0%;"}
![](images/ai-nvidia.png)
:::

::: {.fragment .fade-in}
::: {style="position:absolute;top:35%;left:56.5%;"}
<span style="color: white;font-size: 24px;background-color:#335c67;border-radius: 10px"><strong>Artificial Neural Network (ANN)</strong></span>
:::
:::

::: {.notes}
-   Deep Blue with Kasparow -> 200M of chess positions/seconds -> high speed calculations then probabilistic choices
-   Ex of junk mail detection
-   ChatGPT
:::

## A simple ANN - The Perceptron

Based on an artificial neuron called **threshold logic unit** (TLU)

::: {style="position:absolute;top:20%;left:-10%;font-size:40px;"}
![](images/tlu.jpg){width=800}
:::


::: {style="position:absolute;top:40%;left:60%;font-size:30px;"}
$$
\text{heaviside}(z) = 
\begin{cases}
0 & \text{if } z < 0 \\
1 & \text{if } z \geq 0
\end{cases}
$$
:::

::: {.notes}
Like in a human brain, a neuron needs to be activated -> activation function
:::

## Perceptron composed of one or more TLUs

Every TLU connected to every input = **fully connected layer** or **dense layer**

::: {style="position:absolute;top:20%;left:0%;font-size:40px;"}
![](images/more_tlus.jpg){width=600}
:::

::: {style="position:absolute;top:25%;left:50%;font-size:30px;"}
* $h_{\mathbf{W},\mathbf{b}}(\mathbf{X}) = \phi(\mathbf{X}\mathbf{W} + \mathbf{b})$
* $\mathbf{b} = \text{bias vector}, \text{one value per neuron}$
* $\phi = \text{activation function}$
:::

::: notes
Purpose of bias neurons:

The main purpose of a bias neuron is to shift the activation function left or right, which can be crucial for successful learning. Here's why this is important:
* Flexibility: Bias allows the model to fit the data better by shifting the activation function. Without bias, the function would always pass through the origin (0,0).
* Learning power: Bias increases the capacity of the network to learn complex patterns by providing an additional parameter to adjust.
* Default activation: Bias allows a neuron to have a default "firing" value even when all inputs are zero.
:::

::: {.fragment .fade-in}
::: {style="position:absolute;top:45%;left:50%;font-size:30px;"}
![](images/activation.jpg){width=1200}
:::

::: {style="position:absolute;top:90%;left:50%;font-size:30px;"}
<span style="color: white;font-size: 24px;background-color:#335c67;border-radius: 10px"><strong>$\rightarrow$ backpropagation algorithm (see after)</strong></span>
:::
:::

## Multilayer Perceptron - XOR example

::: {style="position:absolute;top:15%;left:0%;font-size:40px;"}
![](images/multilayer_perceptron.jpg){width=600}
:::

::: {.fragment .fade-in}
::: {style="position:absolute;top:35%;left:55%;font-size:25px;"}
| A | B | A XOR B |
|:-:|:-:|:-------:|
| 0 | 0 |    0    |
| 0 | 1 |    1    |
| 1 | 0 |    1    |
| 1 | 1 |    0    |
:::

::: {style="position:absolute;top:15%;left:75%;font-size:40px;"}
![](images/xor.jpg){width=280}
:::

::: {style="position:absolute;top:80%;left:55%;font-size:20px;"}
ANN with deep stack of hidden layers = **deep neural** network
:::
:::

::: {.notes}
-   Introduce the term of truth table
-   XOR is a function and the CNN tries to approximate this function
-   h1 = (x1 x2) (1 1)^T -3/2
-   h2 = (x1 x2) (1 1)^T -1/2
-   h3 = (h1 h2) (-1 1)^T -1/2 = -h1 + h2 -1/2
:::

## Tweaking parameters to minimize the cost

Famous optimization algorithm - gradient descent (iterative process)

::: {.fragment .fade-in-then-out}
::: {style="position:absolute;top:20%;left:0%;font-size:40px;"}
![](images/cost_function.jpg){width=600}
:::

::: {style="position:absolute;top:25%;left:50%;font-size:30px;"}
* $\boldsymbol{\theta} = \text{parameter vector}$
* $\eta = \text{learning step} = \text{learning rate}$
* $\boldsymbol{\theta}^{(\text{next step})} = \boldsymbol{\theta} - \eta \nabla_{\boldsymbol{\theta}} \text{Cost}(\boldsymbol{\theta})$
* $\text{If Cost}(\boldsymbol{\theta}) = \text{RMSE}(\boldsymbol{\theta})$ 
  * $\text{min Cost}(\boldsymbol{\theta}) = min \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2}$
:::
:::

::: {.fragment .fade-in}
::: {style="position:absolute;top:20%;left:0%;font-size:40px;"}
![](images/early_stop.jpg){width=600}
:::

::: {style="position:absolute;top:25%;left:50%;font-size:30px;"}
* $\boldsymbol{\theta} = \text{parameter vector}$
* $\eta = \text{learning step} = \text{learning rate}$
* $\boldsymbol{\theta}^{(\text{next step})} = \boldsymbol{\theta} - \eta \nabla_{\boldsymbol{\theta}} \text{Cost}(\boldsymbol{\theta})$
* $\text{If Cost}(\boldsymbol{\theta}) = \text{RMSE}(\boldsymbol{\theta})$ 
  * $\text{min Cost}(\boldsymbol{\theta}) = min \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2}$
:::
:::

::: {.notes}
-   The gradient vector points in the direction of steepest ascent
-   The magnitude of the gradient equals the slope in that steepest
-   Explain what is a training, validation and test set
-   Overfitting occurs when a machine learning model learns the training data TOO WELL, including all its noise and random fluctuations, instead of learning the actual underlying pattern (cannot GENERALIZE well)
:::

## Have you ever seen an ANN in action?

[TensorFlow Playground](https://playground.tensorflow.org/){preview-link="true"}

::: {.notes}
-   each neuron cut the space in a linear fashion
-   talk about learning rate (show a case where it is instable -> see case 3, relaunch it several times), activation function, dataset (try several), number of neurons, number of hidden layers, la loss function, ...
-   observe the backpropagation algorithm: 
1) Forward Pass: The neural network makes a prediction based on input data
2) Error Calculation: The network compares its prediction with the correct answer and it calculates how wrong it was (the error)
3) Moving Backward: Starting from the output layer, the algorithm moves backward through the network. At each step, it figures out how much each neuron contributed to the error
4) Weight Updates: The network adjusts the strength (weights) of connections between neurons. Connections that contributed more to the error get adjusted more.
-   case 1: 3rd dataset, no hidden layers (just a linear combination)
-   case 2: 4th dataset, show that we need a more complex network (take all neurons and all layers) to achieve a good result (keep only x1, x2 and sin(x1) and sin(x2) -> manifold learning, un peu comme un cinamon roll, with learning rate = 0.03, RELU and 80% de training data). Actually, make this with 3 hidden layers (8-8-4) and learning rate of 0.1 woth 50% of training data ratio works well.
-   case 3: 2nd dataset, take 3 hidden layers (2-4-2) with only x1 and x2 as outputs, with a ReLU activation function and show the result with a learning rate of 0.03 and 0.1
-   case 4: 4th dataset, same as case 2 but with x1 and x2 squared -> should fail
:::

## Now that we know you've all used chatGPT

::: {style="position:absolute;top:20%;left:0%;font-size:30px;"}
* chatGPT (you know it)
* Claude.ai (20$/month well spent)
* GitHub Copilot (code completion)
* Perplexity (search engine)
* NotebookLM (nice podcasts)
* DeepL (translation)
* Many more
:::

::: {.fragment .fade-in}
::: {style="position:absolute;top:40%;left:50%;"}
<span style="background-color:#335c67; color:white; padding:10px 20px; border-radius:15px; display:inline-block; font-size:40px;">Large Language Models (LLMs)</span>
:::
:::


# Part III {background-color="#335c67"}

::: {style="position:absolute;top:70%;left:0%;font-size:40px;"}
LLMs, what's all the fuss about?
::: 

## LLMs are huge neural networks

-   <span style="color: white;background-color:#335c67;border-radius: 10px"><strong>Billions</strong></span> of parameters (e.g. GPT3 - 175 billions)
-   Specialized in language processing
-   Most famous ones (e.g. chatGPT) are proprietary (i.e. unknown weights)
-   Some models have open weights, in contrast to being fully open source

## Converting text to machine data
 
-   [Tokenization](https://tiktokenizer.vercel.app/?model=text-davinci-003){preview-link="false"} - splitting the text into <span style="color: white;background-color:#335c67;border-radius: 10px"><strong>tokens</strong></span>
- Vectorization - each token receives a vector (e.g. GPT3 $\rightarrow$ 12 288 dimensions)
- Vector with semantic meaning (e.g. `King - Man + Women = Queen`)

::: {.notes}
- test with I love Barack Obama
:::

## Transformers - Attention is all you need

::: {.incremental}
-   GPT - <span style="color: white;background-color:#335c67;border-radius: 10px"><strong>G</strong></span>enerative <span style="color: white;background-color:#335c67;border-radius: 10px"><strong>P</strong></span>retrained <span style="color: white;background-color:#335c67;border-radius: 10px"><strong>T</strong></span>ransformers
-   A transformer is a neural network with many layers
-   <span style="color: white;background-color:#335c67;border-radius: 10px"><strong>Context window</strong></span> - a sequence of tokens used as model input
-   The model outputs a token
-   All tokens pass through the neural network and are modified based on the other tokens
-   `black dog`, the dog vector will be modified to account for the fact that the dog is black
:::

## Trained for probability, not accuracy

::: {.incremental}
-   The output vector is converted into a <span style="color: white;background-color:#335c67;border-radius: 10px"><strong>probability distribution</strong></span>, from which the next token is selected
-   Each generated token becomes part of the new context window (i.e. continuous text generation)
-   Model training - Tries to predict the next token then <span style="color: white;background-color:#335c67;border-radius: 10px"><strong>backpropagation</strong></span> steps in 
-   Trained to be the most probable, not the most accurate
-   <span style="color: white;background-color:#335c67;border-radius: 10px"><strong>Tip</strong></span> - the model is often better at retrieving information at the end of the context window
-   Knowledge learned during training (constant, stored in the model parameters)
-   Knowledge from the context window (different at each interaction)
:::

## Interaction with your new AI companion

1. There is a hidden system prompt that explains to the model that it must simulate a conversation
2. Then a sentence is given by the assistant (e.g. Claude :heart:)
3. A sentence can then be given by the user, the <span style="color: white;background-color:#335c67;border-radius: 10px"><strong>prompt</strong></span>
4. Repeat 2 and 3 until you are satisfied

## Transforming a LLM into a chatbox

-   The model is <span style="color: white;background-color:#335c67;border-radius: 10px"><strong>fine tuned</strong></span> with reinforcement learning human feedback
-   Human feedback - rating model responses as good or bad (see later Bing Chat)
-   Safety training through feedback reduces harmful outputs but may limit model capabilities
-   Models are optimized to generate responses that appear convincing to humans
-   Models are instructed to simulate assistant-human conversations using embedded system prompts
-   Prompt injection risks (i.e. jailbreak) - when model safety controls are bypassed

::: {.fragment .fade-in}
::: {style="position:absolute;top:10%;left:0%;"}
![](images/BingChat.png)
:::
:::

::: {.notes}
- Models are instructed to simulate assistant-human conversations using embedded system prompts (see point 1 in previous slide)
- jailbreak, example of torrent website
:::

# Part IV {background-color="#335c67"}

::: {style="position:absolute;top:70%;left:0%;font-size:40px;"}
Examples and best practices
::: 

## The less the model has to guess the better

[Non exhaustive list](https://platform.openai.com/docs/guides/prompt-engineering/six-strategies-for-getting-better-results){preview-link="false"}

::: {.incremental}
* Write clear instructions
* Provide reference text and/or code 
* Split complex tasks into simpler subtasks
* Give the model time to "think" by asking for a "chain of thought"
* Tell the model what to do, rather than what not to do
* ...
:::

## Some other tips

::: {.incremental}
* Testing multiple models in case of a complex task 
* Thinking a bit how to interact (discussing about equation? The model can understand LaTex)
:::

## Generating image from text (or image)

- cf bing chat, Dall-e, midjourney
- openweigth models exist: flux, stable diffusion... ! Needs a decent GPU to have image generated in a decent time 
- Videos models are quickly improving

## Generating audio/music from text

- NotebookLM (podcast)
- Suno, Udio... (cf COHERENS rap)

## It feels like coding with 4 hands 

## Shiny app built from scratch hello

Amazingly fast.

![](gif/shiny.gif)

# Lunch time ! {background-color="#335c67"}
