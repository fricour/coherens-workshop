---
format: 
  revealjs:
    embed-resources: true # note, chackboard option, even commented, seemed to be an issue with embed-resources..., see https://claudiu.psychlab.eu/post/revealjs-presentations-with-quarto-a-template/
    transition: slide 
    slide-number: c
    progress: true
    theme: [default] # see comment in the custom.scss file
    #css: fonts.css # needed to import the Right Grotesk Compact font
    #theme: default
    margin: 0.1
    width: 1200
    output-file: coherens_workshop.html
    smaller: true
    multiplex: true
editor: source 
  #render-on-save: true
from: markdown+emoji
execute: 
  cache: true # need to put it to false when there is an update of data WITHOUT a change of code
---

# COHERENS workshop #2 {background-color="#335c67"}

Florian Ricour (ECOMOD) <br>
Ludovic Lepers (MFC)

::: footer
Information is current as of November 24, 2024, and subject to change.
:::

# Part I {background-color="#335c67"}

::: {style="position:absolute;top:70%;left:0%;font-size:40px;"}
JupyterHub on ECMWF
::: 

## A clear procedure to get there

- Need an ECMWF account
- Access to local terminal
  1. `tsh login --proxy=jump.ecmwf.int`
  2. `ssh -X hpc-login`
- [Tutorial](https://naturalsciences.sharepoint.com/sites/ECOMOD/Shared%20Documents/Forms/AllItems.aspx?id=%2Fsites%2FECOMOD%2FShared%20Documents%2FECMWF%2Fjupyterlab%5Fecmwf%2Epdf&parent=%2Fsites%2FECOMOD%2FShared%20Documents%2FECMWF) to connect to JupyterHub
- Multiple server [options](https://confluence.ecmwf.int/display/UDOC/Configuring+your+Jupyter+session+on+HPCF+or+ECS) (Profile, CPU number, session duration, ...)

## Let's try [JupyterHub](https://jupyter.org/hub)!

- Demo on [ECMWF JupyterHub](https://jupyterhub.ecmwf.int)
  - Notebook
  - General structure

- HPC [Filesystems](https://confluence.ecmwf.int/display/UDOC/HPC2020%3A+Filesystems)

```{r}
#| label: file-system-table
#| echo: false
#| message: false
#| warning: false
file_system <- c('HOME', 'PERM', 'HPCPERM', 'SCRATCH', 'TMPDIR')
features <- c('Backed up', 'No back up', 'No back up', 'No back up', 'Deleted at the end of session/job')
quota <- c('10 GB', '500 GB', '100 GB*/1 TB', '50 TB*/2 TB', '3 GB by default')

library(tibble)
library(gt)

tb <- tibble(file_system, features, quota)
# Create and format the table
gt_table <- tb %>%
  gt() %>%
  cols_label(
    file_system = "File System",
    features = "Features",
    quota = "Quota"
  ) %>%
  tab_style(
    style = cell_fill(color = "#f0f0f0"),
    locations = cells_body(rows = seq(1, nrow(tb), 2))
  ) %>%
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels()
  ) %>%
  tab_style(
    style = cell_text(color = "navy"),
    locations = cells_body(columns = "file_system")
  ) %>%
  tab_options(
    column_labels.background.color = "#e0e0e0",
    table.border.top.style = "hidden",
    table.border.bottom.style = "hidden"
  ) |>
  tab_header(
    title = "File System Features"
  ) |>
  tab_source_note(
    source_note = "* for users without HPC access such as ECS"
  ) |>
  opt_css(
  css = "
    .gt_table {
      margin-left: 45px !important;
      margin-right: auto !important;
    }
  "
)

# Display the table
gt_table
```

## Conclusion on Part I

::: {.incremental}

- GUI is more user-friendly than a cold heartless terminal
- Take advantage of the HPC filesystem at your disposal
- Recent service (April 2024), you will look so damn cool !
- Session up to 7 days, just go the URL and you're back online
- Use it for other tasks than model simulations
- Save time from heavy downloads/uploads from/to Sharepoint (Erk)
- Preserve your laptop and run big python/R codes on ECMWF
- You want to go home but need to wait for a script to finish ? $\rightarrow$ ECMWF
- I like it and I hope you'll use it !
:::


# Short break {background-color="#335c67"}

# Part II {background-color="#335c67"}

::: {style="position:absolute;top:70%;left:0%;font-size:40px;"}
Artificial Intelligence, how smart is it?
:::

## Who hasn't used ChatGPT here?

::: {.fragment .fade-in-then-out}
<!--![](https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExMjVwdHdtemhlMmdqOGJtY3oyemQ3cXMwMjJ1OG9hNmg4MWxvdHY0byZlcD12MV9naWZzX3NlYXJjaCZjdD1n/EouEzI5bBR8uk/giphy.gif)-->
![](https://media.giphy.com/media/j4lJOuwvAzyRcnWrFi/giphy.gif?cid=790b76114ggiswqfx07dbvx906s4qfy2h30e3j96dvw3qeul&ep=v1_gifs_search&rid=giphy.gif&ct=g)
:::

::: {.fragment .fade-in}
::: {style="position:absolute;top:20%;left:0%;font-size:30px;"}
* chatGPT (you know it)
* Claude.ai (20$/month well spent)
* GitHub Copilot (code completion)
* Perplexity (search engine)
* NotebookLM (nice podcasts)
* DeepL (translation)
* Many more
:::
:::

::: {.fragment .fade-in}
::: {style="position:absolute;top:40%;left:50%;"}
<span style="background-color:#335c67; color:white; padding:10px 20px; border-radius:15px; display:inline-block; font-size:40px;">Artificial intelligence</span>
:::
:::

## AI is a vague terminology

::: {style="position:absolute;top:10%;left:0%;"}
![](images/ai-nvidia.png)
:::

::: {.fragment .fade-in}
::: {style="position:absolute;top:35%;left:55.5%;"}
<span style="color: white;font-size: 24px;background-color:#335c67;border-radius: 10px"><strong>Artificial Neural Network (ANN)</strong></span>
:::
:::

::: {.notes}
-   Deep Blue with Kasparow -> 200M of chess positions/seconds -> high speed calculations then probabilistic choices
-   Ex of junk mail detection
-   ChatGPT
:::

## A simple ANN - The Perceptron

Based on an artificial neuron called <span style="color: white;font-size: 24px;background-color:#335c67;border-radius: 10px">threshold logic unit</span> (TLU)

::: {style="position:absolute;top:20%;left:-10%;font-size:40px;"}
![](images/tlu.jpg){width=800}
:::


::: {style="position:absolute;top:40%;left:60%;font-size:30px;"}
$$
\text{heaviside}(z) = 
\begin{cases}
0 & \text{if } z < 0 \\
1 & \text{if } z \geq 0
\end{cases}
$$
:::

::: {.notes}
Like in a human brain, a neuron needs to be activated -> activation function
:::

## Perceptron composed of one or more TLUs

Every TLU connected to every input = <span style="color: white;font-size: 24px;background-color:#335c67;border-radius: 10px">fully connected layer</span> or dense layer

::: {style="position:absolute;top:20%;left:0%;font-size:40px;"}
![](images/more_tlus.jpg){width=600}
:::

::: {style="position:absolute;top:25%;left:50%;font-size:30px;"}
* $h_{\mathbf{W},\mathbf{b}}(\mathbf{X}) = \phi(\mathbf{X}\mathbf{W} + \mathbf{b})$
* $\mathbf{b} = \text{bias vector}, \text{one value per neuron}$
* $\phi = \text{activation function}$
:::

::: notes
Purpose of bias neurons:

The main purpose of a bias neuron is to shift the activation function left or right, which can be crucial for successful learning. Here's why this is important:
* Flexibility: Bias allows the model to fit the data better by shifting the activation function. Without bias, the function would always pass through the origin (0,0).
* Learning power: Bias increases the capacity of the network to learn complex patterns by providing an additional parameter to adjust.
* Default activation: Bias allows a neuron to have a default "firing" value even when all inputs are zero.
:::

::: {.fragment .fade-in}
::: {style="position:absolute;top:45%;left:50%;font-size:30px;"}
![](images/activation.jpg){width=1200}
:::

::: {style="position:absolute;top:90%;left:50%;font-size:30px;"}
<span style="color: white;font-size: 24px;background-color:#335c67;border-radius: 10px"><strong>$\rightarrow$ backpropagation algorithm (see after)</strong></span>
:::
:::

## Multilayer Perceptron - XOR example

::: {style="position:absolute;top:15%;left:0%;font-size:40px;"}
![](images/multilayer_perceptron.jpg){width=600}
:::

::: {.fragment .fade-in}
::: {style="position:absolute;top:35%;left:55%;font-size:25px;"}
| A | B | A XOR B |
|:-:|:-:|:-------:|
| 0 | 0 |    0    |
| 0 | 1 |    1    |
| 1 | 0 |    1    |
| 1 | 1 |    0    |
:::

::: {style="position:absolute;top:15%;left:75%;font-size:40px;"}
![](images/xor.jpg){width=280}
:::

::: {style="position:absolute;top:80%;left:55%;font-size:20px;"}
ANN with deep stack of hidden layers = <span style="color: white;font-size: 24px;background-color:#335c67;border-radius: 10px">deep neural network</span>
:::
:::

::: {.notes}
-   Introduce the term of truth table
-   XOR is a function and the CNN tries to approximate this function
-   h1 = (x1 x2) (1 1)^T -3/2
-   h2 = (x1 x2) (1 1)^T -1/2
-   h3 = (h1 h2) (-1 1)^T -1/2 = -h1 + h2 -1/2
:::

## Tweaking parameters to minimize the cost

Famous optimization algorithm - gradient descent (iterative process)

::: {.fragment .fade-in-then-out}
::: {style="position:absolute;top:20%;left:0%;font-size:40px;"}
![](images/cost_function.jpg){width=600}
:::

::: {style="position:absolute;top:25%;left:50%;font-size:30px;"}
* $\boldsymbol{\theta} = \text{parameter vector}$
* $\eta = \text{learning step} = \text{learning rate}$
* $\boldsymbol{\theta}^{(\text{next step})} = \boldsymbol{\theta} - \eta \nabla_{\boldsymbol{\theta}} \text{Cost}(\boldsymbol{\theta})$
* $\text{If Cost}(\boldsymbol{\theta}) = \text{RMSE}(\boldsymbol{\theta})$ 
  * $\text{min Cost}(\boldsymbol{\theta}) = min \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2}$
:::
:::

::: {.fragment .fade-in}
::: {style="position:absolute;top:20%;left:0%;font-size:40px;"}
![](images/early_stop.jpg){width=600}
:::

::: {style="position:absolute;top:25%;left:50%;font-size:30px;"}
* $\boldsymbol{\theta} = \text{parameter vector}$
* $\eta = \text{learning step} = \text{learning rate}$
* $\boldsymbol{\theta}^{(\text{next step})} = \boldsymbol{\theta} - \eta \nabla_{\boldsymbol{\theta}} \text{Cost}(\boldsymbol{\theta})$
* $\text{If Cost}(\boldsymbol{\theta}) = \text{RMSE}(\boldsymbol{\theta})$ 
  * $\text{min Cost}(\boldsymbol{\theta}) = min \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2}$
:::
:::

::: {.notes}
-   The gradient vector points in the direction of steepest ascent
-   The magnitude of the gradient equals the slope in that steepest
-   Explain what is a training, validation and test set
-   Overfitting occurs when a machine learning model learns the training data TOO WELL, including all its noise and random fluctuations, instead of learning the actual underlying pattern (cannot GENERALIZE well)
:::

## Have you ever seen an ANN in action?

[TensorFlow Playground](https://playground.tensorflow.org/){preview-link="true"}

::: {.notes}
-   each neuron cut the space in a linear fashion
-   talk about learning rate (show a case where it is instable -> see case 3, relaunch it several times), activation function, dataset (try several), number of neurons, number of hidden layers, la loss function, ...
-   observe the backpropagation algorithm: 
1) Forward Pass: The neural network makes a prediction based on input data
2) Error Calculation: The network compares its prediction with the correct answer and it calculates how wrong it was (the error)
3) Moving Backward: Starting from the output layer, the algorithm moves backward through the network. At each step, it figures out how much each neuron contributed to the error
4) Weight Updates: The network adjusts the strength (weights) of connections between neurons. Connections that contributed more to the error get adjusted more.
-   case 1: 3rd dataset, no hidden layers (just a linear combination)
-   case 2: 4th dataset, show that we need a more complex network (take all neurons and all layers) to achieve a good result (keep only x1, x2 and sin(x1) and sin(x2) -> manifold learning, un peu comme un cinamon roll, with learning rate = 0.03, RELU and 80% de training data). Actually, make this with 3 hidden layers (8-8-4) and learning rate of 0.1 woth 50% of training data ratio works well.
-   case 3: 2nd dataset, take 3 hidden layers (2-4-2) with only x1 and x2 as outputs, with a ReLU activation function and show the result with a learning rate of 0.03 and 0.1
-   case 4: 4th dataset, same as case 2 but with x1 and x2 squared -> should fail
:::

## Now that we know you've all used chatGPT

::: {style="position:absolute;top:20%;left:0%;font-size:30px;"}
* chatGPT (you know it)
* Claude.ai (20$/month well spent)
* GitHub Copilot (code completion)
* Perplexity (search engine)
* NotebookLM (nice podcasts)
* DeepL (translation)
* Many more
:::

::: {.fragment .fade-in}
::: {style="position:absolute;top:40%;left:50%;"}
<span style="background-color:#335c67; color:white; padding:10px 20px; border-radius:15px; display:inline-block; font-size:40px;">Large Language Models (LLMs)</span>
:::
:::


# Part III {background-color="#335c67"}

::: {style="position:absolute;top:70%;left:0%;font-size:40px;"}
LLMs, what's all the fuss about?
::: 

## LLMs are huge neural networks

-   <span style="color: white;background-color:#335c67;border-radius: 10px">Billions</span> of parameters (e.g. GPT3 - 175 billions)
-   Specialized in language processing
-   Most famous ones (e.g. GPT3) are proprietary (i.e. unknown weights)
-   Some models have open weights, in contrast to being fully open source

## Converting text to machine data
 
-   [Tokenization](https://tiktokenizer.vercel.app/?model=text-davinci-003){preview-link="false"} - splitting the text into <span style="color: white;background-color:#335c67;border-radius: 10px">tokens</span>
- Vectorization - each token receives a vector (e.g. GPT3 $\rightarrow$ 12 288 dimensions)
- Vector with semantic meaning (e.g. `King - Man + Women = Queen`)

::: {.notes}
- test with I love Barack Obama
:::

## Transformers - Attention is all you need

::: {.incremental}
-   GPT - <span style="color: white;background-color:#335c67;border-radius: 10px">G</span>enerative <span style="color: white;background-color:#335c67;border-radius: 10px">P</span>retrained <span style="color: white;background-color:#335c67;border-radius: 10px">T</span>ransformers
-   A transformer is a neural network with many layers
-   <span style="color: white;background-color:#335c67;border-radius: 10px">Context window</span> - a sequence of tokens used as model input
-   The model outputs a token
-   All tokens pass through the neural network and are modified based on the other tokens
-   `black dog`, the dog vector will be modified to account for the fact that the dog is black
:::

::: {.notes}
Transformers treat each word in parallel
:::

## Trained for probability, not truth

::: {.incremental}
-   The output vector is converted into a <span style="color: white;background-color:#335c67;border-radius: 10px">probability distribution</span>, from which the next token is selected
-   Each generated token becomes part of the new context window (i.e. continuous text generation)
-   Model training - Tries to predict the next token then <span style="color: white;background-color:#335c67;border-radius: 10px">backpropagation</span> steps in 
-   Trained to be the most probable, not the most accurate
-   Knowledge learned during training (constant, stored in the model parameters)
-   Knowledge from the context window (different at each interaction)
:::

## Interaction with your new AI companion

1. There is a hidden system prompt that explains to the model that it must simulate a conversation
2. Then a sentence is given by the assistant (e.g. Claude :heart:)
3. A sentence can then be given by the user, the <span style="color: white;background-color:#335c67;border-radius: 10px">prompt</span>
4. Repeat 2 and 3 until you are satisfied

## Transforming a LLM into a chatbot

-   The model is <span style="color: white;background-color:#335c67;border-radius: 10px">fine tuned</span> with reinforcement learning human feedback
-   Human feedback - rating model responses as good or bad (see later Bing Chat)
-   Safety training through feedback reduces harmful outputs but may limit model capabilities
-   Models are optimized to generate responses that appear convincing to humans
-   Models are instructed to simulate assistant-human conversations using embedded system prompts
-   Prompt injection risks (i.e. jailbreak) - when model safety controls are bypassed

::: {.fragment .fade-in}
::: {style="position:absolute;top:10%;left:0%;"}
![](images/BingChat.png)
:::
:::

::: {.notes}
- Models are instructed to simulate assistant-human conversations using embedded system prompts (see point 1 in previous slide)
- jailbreak, example of torrent website
:::

# Part IV {background-color="#335c67"}

::: {style="position:absolute;top:70%;left:0%;font-size:40px;"}
Examples and best practices
::: 

# The science of prompting {background-color="#335c67"}

## The less the model has to guess the better

[Non exhaustive list](https://platform.openai.com/docs/guides/prompt-engineering/six-strategies-for-getting-better-results){preview-link="false"}

::: {.content-visible when-format="html"}
<details>
<summary style="color: black;">Write clear instructions</summary>

:x: *Help me with my project* <br>
:white_check_mark: *Help me write the introduction for a school project about climate change*

</details>
:::

::: {.content-visible when-format="html"}
<details>
<summary style="color: black;">Split complex tasks into simpler subtasks</summary>

:x: *Analyze a dataset and create a visualization*<br>
:white_check_mark: *Subtask 1: Load and clean the data*<br>
:white_check_mark: *Subtask 2: Perform statistical analysis*<br>
:white_check_mark: *Subtask 3: Create appropriate visualizations*<br>
:white_check_mark: *Subtask 4: Write up insights*<br><br>
Advantage: Lower error rates compared to using a single query to perform the whole task.

</details>
:::

::: {.content-visible when-format="html"}
<details>
<summary style="color: black;">Give the model time to think with chain of thought reasoning (i.e. step-by-step reasoning)</summary>

:question: *Problem: If it takes 6 workers 4 days to build a wall, how long would it take 8 workers?* <br>
:bulb: *As you solve this problem:*<br>
:white_check_mark: *First, state what information is relevant*<br>
:white_check_mark: *Then, explain how you'll approach it*<br>
:white_check_mark: *Show each calculation*<br>
:white_check_mark: *Check if your answer makes sense*<br>
:white_check_mark: *Explain why your answer is reasonable*<br><br>

Note: Task decomposition can involve parallel processing, while chain of thought is typically sequential.

</details>
:::

::: {.content-visible when-format="html"}
<details>
<summary style="color: black;">Role-based prompts</summary>
:white_check_mark: *As a deep-sea biologist: A submersible has discovered massive die-offs of tube worms at 2000m depth near a hydrothermal vent that was previously thriving. What would be your initial assessment?*<br>
:white_check_mark: *You're a coastal oceanographer working with a beach community: Local swimmers report unusual purple-blue floating organisms appearing in large numbers during the past week. What's your response?*<br><br>
Advantage: This helps give more specialized answers.
</details>
:::

::: {.content-visible when-format="html"}
<details>
<summary style="color: black;">Provide references</summary>
:white_check_mark: Upload documents<br>
:white_check_mark: Provide text and/or code examples<br>
:white_check_mark: Even screenshots work well<br><br>
Advantage: It reduces fake answers and match text and/or code style
</details>
:::

::: {.content-visible when-format="html"}
<details>
<summary style="color: black;">Tell the model what to do, rather than what not to do</summary>

:x: *Don't use complex technical terms when explaining photosynthesis*<br>
:white_check_mark: *Explain photosynthesis using everyday language and familiar examples like sunlight helping plants make their food*

</details>
:::

::: {.content-visible when-format="html"}
<details>
<summary style="color: black;">Context matters - start a new chat if you change topics</summary>

:bulb: Remember the continuous text generation

</details>
:::

## Additional prompting tips

* Testing multiple models if needed (Claude 3.5 Sonnet, Claude 3 Opus, GPT-4o, GPT-o1, ...)
* Think about your interaction with the assistant (e.g. text or code or even screenshot as input)
* 

::: {.notes}
* Example - You want to discuss about an equation, use LaTex instead of giving a screenshot, the models understands LaTex
:::

# Images, audio and notes {background-color="#335c67"} 

## Image generation from text or image

* [Bing Chat](https://www.bing.com/images/create), [Dall-E](https://openai.com/index/dall-e-3/), [Midjourney](https://www.midjourney.com/home)
* Open weights models - [Flux](https://blackforestlabs.ai/), [Stable Diffusion](https://stability.ai/) (you'll need a decent GPU)
* [Perchance AI](https://perchance.org/ai-text-to-image-generator) (free tool)
* [Prompt generator for image](https://huggingface.co/spaces/gokaygokay/FLUX-Prompt-Generator)
* Video models are starting to be really good ([Runway Gen-2](https://www.youtube.com/watch?v=71-yl2SYQpg), [ChatGPT Sora](https://www.youtube.com/watch?v=HK6y8DAPN_0))

::: {.notes}
short prompt: Ocean wind farms and solar panels in between. Focus on the solar panels. In the North sea. Image fit for scientific publication.
long prompt: Vast North Sea landscape with towering wind turbines and expansive solar panels stretching into the horizon, capturing the serene yet powerful energy of the ocean. Clear blue sky with wispy clouds, sunlight casting soft shadows. Solar panels glisten under the natural daylight, reflecting the azure waters. Cinematic wide shot with deep focus, inspired by the visual style of Christopher Nolan. Crisp, high-resolution image with minimal film grain, cool color grading emphasizing the technology and sustainability.
:::

## Audio generation from text

- [NotebookLM](https://notebooklm.google/) - podcast
{{< audio file="audio/converse_podcast.mp3" caption="Century-scale carbon sequestration flux throughout the ocean by the biological pump">}}
- [Suno](https://suno.com/), [udio](https://www.udio.com/) - music
{{< audio file="audio/coherens.mp3" caption="Based on COHERENS's documentation">}}

## Getting the info that matters the most

with [NotebookLM](https://notebooklm.google/), powered by Gemini 1.5

* Upload your sources (PDFs, websites, YouTube videos, Google Docs/Slides, ...)
* NotebookLM will summarize them and make connections (suggest questions) between topics
* Exact quotes from sources (relies only on uploaded documents)
* Multimodal - can analyse images and/or plots as well
* Confidential - data not used to train the model
* Helpful to summarize papers, anticipate questions, prepare presentations, ...

::: {.notes}
prepare presentations? At least you can check the outline of the presentation and see if it makes sense at that your key message is represented.
:::

# Coding can be fast, really fast {background-color="#335c67"}

## It feels like coding with 4 hands 

## Shiny app built from scratch hello

Amazingly fast.

![](gif/shiny.gif)

# Lunch time ! {background-color="#335c67"}
